[pytest]
# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Output options
addopts =
    -ra
    -q
    --strict-markers
    --strict-config
    --cov=src
    --cov-report=term-missing:skip-covered
    --cov-report=html
    --cov-fail-under=80

# Benchmark configuration
benchmark_min_rounds = 5
benchmark_max_time = 10.0
benchmark_warmup = true
benchmark_warmup_iterations = 2
benchmark_disable_gc = true
benchmark_sort = mean
benchmark_storage = file://.benchmarks
benchmark_autosave = true
benchmark_save_data = true
benchmark_compare_fail = mean:10%
benchmark_group_by = group,param,name

# Markers
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    performance: marks tests as performance benchmarks
    us1: marks tests for User Story 1
    us2: marks tests for User Story 2
    us3: marks tests for User Story 3
    us4: marks tests for User Story 4

# Coverage
[coverage:run]
source = src
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */site-packages/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
